{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ### import file from another folder ###\n",
    "# import sys\n",
    "# # insert at 1, 0 is the script path (or '' in REPL)\n",
    "# sys.path.insert(1, '../Python-Save-Plots')\n",
    "# import SaveFigAsPDF_PGF as sF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0, Train loss: 0.96195 Test loss: 289.06946 Train RMSE: 0.49777 Test RMSE: 0.61371\n",
      "Epoch:  100, Train loss: -1.11122 Test loss: -303.02802 Train RMSE: 0.07914 Test RMSE: 0.07428\n",
      "Epoch:  200, Train loss: -1.28338 Test loss: -357.36020 Train RMSE: 0.06688 Test RMSE: 0.06688\n",
      "Epoch:  300, Train loss: -1.34681 Test loss: -365.35800 Train RMSE: 0.06495 Test RMSE: 0.06639\n",
      "Epoch:  400, Train loss: -1.39242 Test loss: -374.52441 Train RMSE: 0.06286 Test RMSE: 0.06542\n",
      "Epoch:  500, Train loss: -1.41243 Test loss: -375.89716 Train RMSE: 0.06077 Test RMSE: 0.06586\n",
      "Epoch:  600, Train loss: -1.43620 Test loss: -380.06577 Train RMSE: 0.05967 Test RMSE: 0.06576\n",
      "Epoch:  700, Train loss: -1.44441 Test loss: -379.09473 Train RMSE: 0.05863 Test RMSE: 0.06521\n",
      "Epoch:  800, Train loss: -1.45999 Test loss: -380.75540 Train RMSE: 0.05792 Test RMSE: 0.06493\n",
      "Epoch:  900, Train loss: -1.46602 Test loss: -383.13898 Train RMSE: 0.05763 Test RMSE: 0.06503\n",
      "Epoch: 1000, Train loss: -1.47772 Test loss: -383.64749 Train RMSE: 0.05694 Test RMSE: 0.06449\n",
      "Epoch: 1100, Train loss: -1.47238 Test loss: -379.05554 Train RMSE: 0.05674 Test RMSE: 0.06560\n",
      "Epoch: 1200, Train loss: -1.47959 Test loss: -381.46246 Train RMSE: 0.05653 Test RMSE: 0.06520\n",
      "Epoch: 1300, Train loss: -1.48485 Test loss: -383.89471 Train RMSE: 0.05634 Test RMSE: 0.06418\n",
      "Epoch: 1400, Train loss: -1.48740 Test loss: -381.20850 Train RMSE: 0.05590 Test RMSE: 0.06503\n",
      "Epoch: 1499, Train loss: -1.49308 Test loss: -385.30908 Train RMSE: 0.05589 Test RMSE: 0.06389\n",
      "Train log. lik. = 1.54876 +/- 0.00000\n",
      "Test  log. lik. = 384.23505 +/- 0.00000\n",
      "Train RMSE      = 0.05562 +/- 0.00000\n",
      "Test  RMSE      = 0.06429 +/- 0.00000\n",
      "[0.51958436 0.51958436 0.51958436] [0.05161075 0.05161075 0.05161075] [0.01510713 0.01510713 0.01510713] [0.05379825 0.05379825 0.05379825]\n",
      "[0.52408457] [[0.52455556]\n",
      " [0.5266612 ]\n",
      " [0.5210369 ]]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "wght_decay, learn_rate = 0.1, 4e-3\n",
    "train_data_ratio, nb_epochs = 0.8, 1500\n",
    "p_drop_rate = 0.1\n",
    "nb_units = 50\n",
    "# nb_units = [10, 20, 30, 50, 100]\n",
    "# p_drop_rate = [0.05, 0.075, 0.1, 0.15, 0.2, 0.3, 0.4]\n",
    "\n",
    "plot_history = False # plot loss and rmse vs epochs if True\n",
    "\n",
    "def normalize_max_min(data, data_max, data_min):\n",
    "    return (data - data_min) / (data_max - data_min)\n",
    "\n",
    "def denormalize_max_min(data, data_max, data_min):\n",
    "    return data * (data_max - data_min) + data_min\n",
    "\n",
    "def measurements_and_training_data(num_parts=25, ratio_=1.0):\n",
    "    \"\"\"\n",
    "    num_parts: is the number of parts to be analyzed, ratio_: is the ratio of the data to be used as training\n",
    "    This function load the dataset for the bond length of each part (1 to num_parts)\n",
    "    \"\"\"\n",
    "    # =================================================================================================================== %\n",
    "    # _______________________________________________       INPUTS     ___________________________________________________\n",
    "    # =================================================================================================================== %\n",
    "    wth = 0.8  # width of filaments [m]\n",
    "\n",
    "    # =================================================================================================================== %\n",
    "    # ___________________________________        IMPORT Measured BOND LENGTH DATA     ____________________________________\n",
    "    # =================================================================================================================== %\n",
    "    # Directory where measurements are stored\n",
    "    dir_measurements = \"./data/sets\"\n",
    "\n",
    "    # total number of sets\n",
    "    total_sets = 3\n",
    "\n",
    "    # IMPORT Measured BOND LENGTH DATA\n",
    "    start_part_id = 1  # starting ID part\n",
    "\n",
    "    total_parts = np.arange(start_part_id, num_parts + 1, dtype='int32')\n",
    "    index_to_remove = [13, 20]  # remove parts that do not have measurements, i.e., 5,14,21\n",
    "    total_parts = np.delete(total_parts, index_to_remove)  # remove elements\n",
    "\n",
    "    ctr = 0  # counter of parts\n",
    "    measured_bl_row = []  # list stores bond length (BL) data of all parts\n",
    "    inp = []  # list storing training data inputs\n",
    "\n",
    "    # measured_bl_sets = [] # list of BL sets\n",
    "\n",
    "    for set_id in range(1, total_sets + 1):  # loop over each part set\n",
    "        measured_bl_eachSet = []\n",
    "        for part_id in total_parts:  # loop over each manufactured part ID\n",
    "\n",
    "            # Read Process parameters & bond length measurements\n",
    "            bl_file = dir_measurements + \"/Round\" + str(set_id) + \"/PPSet\" + str(part_id) + \".xlsx\"\n",
    "\n",
    "            df = pd.read_excel(bl_file, header=None)  # read the excel file\n",
    "\n",
    "            num_layers = df.iloc[2, 0]  # # of layers\n",
    "            num_interfaces = df.iloc[-2, 1]  # # of interfaces/layer\n",
    "            num_interfaces_of_a_part = int(num_layers * num_interfaces)  # num. of interfaces of that part\n",
    "\n",
    "            # save printer temperature, speed, height input for each part\n",
    "            t_n = df.iloc[0, 1]  # Printer nozzle temperature(ยบC)\n",
    "            v_p = df.iloc[0, 3]  # printer speed mm / s\n",
    "            hth = df.iloc[0, 5]  # layer height[mm]\n",
    "            t_n = float(t_n)\n",
    "            v_p = float(v_p)\n",
    "\n",
    "            # print('T_N, v_p, height:', t_n, v_p, hth, \"\\n\")\n",
    "\n",
    "            raw_measuredbl = df.iloc[2:-1, 3]  # measured bond lengths between each interface\n",
    "            raw_measuredbl = raw_measuredbl.astype('float32')\n",
    "\n",
    "            # reshape the measured bond length array & convert to numpy ndarray\n",
    "            reshaped_measured_bl = raw_measuredbl.values.reshape(num_interfaces, num_layers, order='F').copy()\n",
    "\n",
    "            # first column is 1st layer and soon(each row is each interface bond length, BL)\n",
    "            measured_bl = np.fliplr(reshaped_measured_bl).T  # flip matrix left to right\n",
    "\n",
    "            # store measured BL data of all parts in order reshaped in row\n",
    "            measured_bl_row.append([measured_bl.reshape(num_interfaces_of_a_part).copy()])\n",
    "            measured_bl_eachSet.append([measured_bl.reshape(num_interfaces_of_a_part).copy()])\n",
    "\n",
    "            # if part_id==5:\n",
    "            #     print(measured_bl.T)\n",
    "\n",
    "            # Prediction inputs are x & y coordinates of vertical bond length locations\n",
    "            # x, y coordinate of layer 1 & interface 1(vertical)\n",
    "            ycoord = 0.5 * hth  # 0.5*height of a layer in mm\n",
    "            iki_y = ycoord * 2\n",
    "\n",
    "            # store inputs for GP(model disrepancy at each interface)\n",
    "            for jj in range(1, num_layers + 1):\n",
    "                for ii in range(1, num_interfaces + 1):\n",
    "                    # use x & y coordinates of vertical bonds as training data for the GP\n",
    "                    # Inp =[ Temperature, speed, height, x, y ]\n",
    "                    inp.append([t_n, v_p, hth, ii * wth, ycoord + (jj - 1) * iki_y])\n",
    "\n",
    "            ctr += 1  # increment counter to keep track of total number of parts analyzed\n",
    "\n",
    "    #     # to save 3 sets of data on 3 columns to analyze mean and std of data for aleatoric\n",
    "    #     measured_bl_sets.append(measured_bl_eachSet)\n",
    "    # measured_bl_set1 = np.concatenate(measured_bl_sets[0], axis=1)\n",
    "    # measured_bl_set1 = measured_bl_set1.T  # transpose s.t. the number of rows matches Inp\n",
    "    # measured_bl_set2 = np.concatenate(measured_bl_sets[1], axis=1)\n",
    "    # measured_bl_set2 = measured_bl_set2.T  # transpose s.t. the number of rows matches Inp\n",
    "    # measured_bl_set3 = np.concatenate(measured_bl_sets[2], axis=1)\n",
    "    # measured_bl_set3 = measured_bl_set3.T  # transpose s.t. the number of rows matches Inp\n",
    "    # measured_bl_setsAll = np.hstack((measured_bl_set1,measured_bl_set2,measured_bl_set3))\n",
    "\n",
    "    # Inp: stored inputs for Gaussian process\n",
    "    # (model disrepancy at each interface): [T_N, v_p, hth, x, y]\n",
    "    # Convert built Python lists to a Numpy array.\n",
    "    inp = np.array(inp, dtype='float32')\n",
    "\n",
    "    # concatenating different size arrays stored in a list\n",
    "    measured_bl_row = np.concatenate(measured_bl_row, axis=1)\n",
    "    measured_bl_row = measured_bl_row.T  # transpose s.t. the number of rows matches Inp\n",
    "\n",
    "    # Normalize training data\n",
    "    # inp = (inp - inp.mean(axis=0)) / inp.std(axis=0)  # take mean of each column\n",
    "    # measured_bl_row = (measured_bl_row - measured_bl_row.mean(axis=0)) / measured_bl_row.std(axis=0)  # take mean of each column\n",
    "\n",
    "    alldata = np.hstack((inp, measured_bl_row))  # stack 2 numpy arrays column-wise\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    #               Random Permutation of Training Data\n",
    "    # -------------------------------------------------------------------------\n",
    "    nl = inp.shape[0]  # size of training data\n",
    "\n",
    "    # randomly select RatioToBeUsed to be training set for GP model\n",
    "    num_train = round(ratio_ * nl)\n",
    "    idx_ = np.random.permutation(nl)\n",
    "    # idx_ = np.arange(nl)  # do not do random permutation\n",
    "\n",
    "    # Use the first RatioToBeUsed to train the model\n",
    "    idx_train = idx_[0:num_train]\n",
    "    # all_data_train = alldata[idx_train, :]\n",
    "\n",
    "    # mean_of_data = all_data_train.mean(axis=0)\n",
    "    # std_of_data = all_data_train.std(axis=0)\n",
    "    # all_data_train = (all_data_train - mean_of_data) / std_of_data  # take mean of each column\n",
    "\n",
    "    # The (1-RatioToBeUsed) will be used to test the model\n",
    "    idx_test = idx_[(num_train + 1):]\n",
    "    x_test = alldata[idx_test, :-1]\n",
    "    y_test = alldata[idx_test, -1]\n",
    "\n",
    "    return alldata, x_test, y_test\n",
    "\n",
    "# load training and test data\n",
    "train_data, x_tst, y_tst = measurements_and_training_data()\n",
    "\n",
    "x = train_data[:, :-1]  # training data, for all but last column\n",
    "y = train_data[:, -1]  # measurements of the training data, last column\n",
    "\n",
    "# Replace the values of  bond length (BL) that are less than 0.1 with 0.\n",
    "# The reason for this is that the BL values were supposed to be zero,\n",
    "# for those interfaces but not recorded so.\n",
    "y[y < 0.1] = 0\n",
    "\n",
    "# save max and min values of x and y\n",
    "max_x, min_x, max_y, min_y = x.max(axis=0), x.min(axis=0), y.max(axis=0), y.min(axis=0)\n",
    "\n",
    "# import pickle\n",
    "# with open('maxmin.pickle', 'wb') as f:\n",
    "#     pickle.dump([max_x, min_x, max_y, min_y], f)\n",
    "\n",
    "# normalize data\n",
    "x = normalize_max_min(x, max_x, min_x)\n",
    "y = normalize_max_min(y, max_y, min_y)\n",
    "\n",
    "# stack input and output data\n",
    "train_data = np.column_stack((x, y))\n",
    "# batch_size = train_data.shape[0]\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "\n",
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "\n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "\n",
    "def log_gaussian_loss(output, target, sigma, no_dim):\n",
    "    exponent = -0.5 * (target - output) ** 2 / sigma ** 2\n",
    "    log_coeff = -no_dim * torch.log(sigma) - 0.5 * no_dim * np.log(2 * np.pi)\n",
    "    return - (log_coeff + exponent).sum()\n",
    "\n",
    "\n",
    "class MC_Dropout_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_units, drop_prob):\n",
    "        super(MC_Dropout_Model, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        # network with two hidden and one output layer\n",
    "        self.layer1 = nn.Linear(input_dim, num_units)\n",
    "        self.layer2 = nn.Linear(num_units, num_units)\n",
    "        # self.layer3 = nn.Linear(num_units, num_units)\n",
    "        # self.layer4 = nn.Linear(num_units, num_units)\n",
    "        self.layer3 = nn.Linear(num_units, 2 * output_dim)\n",
    "\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = F.dropout(x, p=self.drop_prob, training=True)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.activation(x)\n",
    "        x = F.dropout(x, p=self.drop_prob, training=True)\n",
    "        #\n",
    "        # x = self.layer3(x)\n",
    "        # x = self.activation(x)\n",
    "        # x = F.dropout(x, p=self.drop_prob, training=True)\n",
    "        #\n",
    "        # x = self.layer4(x)\n",
    "        # x = self.activation(x)\n",
    "        # x = F.dropout(x, p=self.drop_prob, training=True)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MC_Dropout_Wrapper:\n",
    "    def __init__(self, network, learn_rate, batch_size, weight_decay):\n",
    "        self.learn_rate = learn_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.network = network\n",
    "        self.network.cuda()\n",
    "\n",
    "        # self.optimizer = torch.optim.SGD(self.network.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "        self.loss_func = log_gaussian_loss\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        x, y = to_variable(var=(x, y), cuda=True)\n",
    "\n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        output = self.network(x)\n",
    "\n",
    "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_loss_and_rmse(self, x, y, num_samples):\n",
    "        x, y = to_variable(var=(x, y), cuda=True)\n",
    "\n",
    "        means, stds = [], []\n",
    "        for i in range(num_samples):\n",
    "            output = self.network(x)\n",
    "            means.append(output[:, :1])\n",
    "            stds.append(output[:, 1:].exp())\n",
    "\n",
    "        means, stds = torch.cat(means, dim=1), torch.cat(stds, dim=1)\n",
    "        mean = means.mean(dim=-1)[:, None]\n",
    "        std = ((means.var(dim=-1) + stds.mean(dim=-1) ** 2) ** 0.5)[:, None]\n",
    "        loss = self.loss_func(mean, y, std, 1)\n",
    "\n",
    "        rmse = ((mean - y) ** 2).mean() ** 0.5\n",
    "\n",
    "        return loss.detach().cpu(), rmse.detach().cpu()\n",
    "\n",
    "\n",
    "def train_mc_dropout(data, drop_prob, ratio_train_data, num_epochs, num_units, learn_rate, weight_decay, log_every,\n",
    "                     num_samples):\n",
    "    in_dim = data.shape[1] - 1\n",
    "    train_logliks, test_logliks = [], []\n",
    "    train_rmses, test_rmses = [], []\n",
    "\n",
    "    # history_loss, history_loss_test, history_rmse, history_rmse_test = [], [], [], []\n",
    "\n",
    "#     # -------------------------------------------------------------------------\n",
    "#     #               Random Permutation of Training Data\n",
    "#     # -------------------------------------------------------------------------\n",
    "#     nl = data.shape[0]  # size of training data\n",
    "\n",
    "#     # randomly select RatioToBeUsed to be training set for GP model\n",
    "#     num_train = round(ratio_train_data * nl)\n",
    "#     idx_ = np.random.permutation(nl)\n",
    "#     # idx_ = np.arange(nl)  # do not do random permutation\n",
    "\n",
    "#     # Use the first RatioToBeUsed to train the model\n",
    "#     idx_train = idx_[0:num_train]\n",
    "\n",
    "#     x_train, y_train = data[idx_train, :in_dim], data[idx_train, in_dim:]\n",
    "\n",
    "#     # The (1-RatioToBeUsed) will be used to test the model\n",
    "#     idx_test = idx_[(num_train + 1):]\n",
    "#     x_test, y_test = data[idx_test, :in_dim], data[idx_test, in_dim:]\n",
    "\n",
    "#     print(x_train.shape, x_test.shape)\n",
    "\n",
    "    x, y = data[:, :in_dim], data[:, in_dim:]\n",
    "\n",
    "    # Find index where elements change value numpy (temperature value first column)\n",
    "    # sorted_test_data[:-1, 0] != sorted_test_data[1:, 0] or np.where(v[:-1] != v[1:])[0] (for indices)\n",
    "    # unique_temp_index: indices up to which same temperature data is sorted, every index value is the last data of that\n",
    "    # part that is printed with that temperature\n",
    "    unique_temp_index = np.where(x[:-1, 0] != x[1:, 0])[0] + 1\n",
    "    unique_temp_index = np.insert(unique_temp_index,0,0)\n",
    "    \n",
    "    x_denorm = denormalize_max_min(x, max_x, min_x)\n",
    "#     print(x_train_denorm[unique_temp_index[:],:],unique_temp_index[-1])\n",
    "    \n",
    "    T = 266 # temperature of the part to be used for leave-one-out\n",
    "#     print(x_train_denorm[unique_temp_index[:],0] == T)\n",
    "#     print(unique_temp_index[x_train_denorm[unique_temp_index[:],0] == T])\n",
    "#     print(unique_temp_index[:])\n",
    "    # using enumerate() + list comprehension \n",
    "    # to return true indices.\n",
    "    test_list = x_denorm[unique_temp_index[:],0] == T\n",
    "    res = [i for i, val in enumerate(test_list) if val]\n",
    "    \n",
    "    start_index = unique_temp_index[res]\n",
    "    # using list comprehension, adding 1 to each element except last one\n",
    "    res = [k + 1 for k in res[:-1]]\n",
    "    end_index = unique_temp_index[res]\n",
    "    end_index = np.append(end_index, x.shape[0])\n",
    "    \n",
    "#     print(res, start_index, end_index)\n",
    "\n",
    "    x_test, y_test = [], []\n",
    "    indices_to_remove=[]\n",
    "    x_test.append(data[start_index[0]:end_index[0], :in_dim])\n",
    "    for ii in range(len(start_index)):\n",
    "        y_test.append(data[start_index[ii]:end_index[ii], in_dim:])\n",
    "#         data_new = np.delete(data,np.s_[start_index[ii]:end_index[ii]],axis=0)\n",
    "        indices_to_remove.append([range(start_index[ii],end_index[ii])])\n",
    "    \n",
    "    # Convert built Python lists to a Numpy array.\n",
    "    x_test = np.array(x_test, dtype='float32')\n",
    "    y_test = np.array(y_test, dtype='float32')\n",
    "\n",
    "    \n",
    "        \n",
    "    data_new = np.delete(data,np.s_[indices_to_remove],axis=0)\n",
    "    \n",
    "    x_train, y_train = data_new[:, :in_dim], data_new[:, in_dim:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    net = MC_Dropout_Wrapper(\n",
    "        network=MC_Dropout_Model(input_dim=in_dim, output_dim=1, num_units=num_units, drop_prob=drop_prob),\n",
    "        learn_rate=learn_rate, batch_size=batch_size, weight_decay=weight_decay)\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        loss = net.fit(x_train, y_train)\n",
    "\n",
    "        # train_loss, rmse_train = net.get_loss_and_rmse(x_train, y_train, num_samples=num_samples)\n",
    "        # rmse_train = rmse_train.cpu().data.numpy()\n",
    "        #\n",
    "        # test_loss, rmse_test = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
    "        # test_loss, rmse_test = test_loss.cpu().data.numpy(), rmse_test.cpu().data.numpy()\n",
    "\n",
    "        # history_loss.append(loss.cpu().data.numpy() / len(x_train))\n",
    "        # history_loss_test.append(test_loss / len(x_test))\n",
    "        # history_rmse.append(rmse_train)\n",
    "        # history_rmse_test.append(rmse_test)\n",
    "\n",
    "        if i % log_every == 0 or i == num_epochs - 1:\n",
    "            train_loss, rmse_train = net.get_loss_and_rmse(x_train, y_train, num_samples=num_samples)\n",
    "            rmse_train = rmse_train.cpu().data.numpy()\n",
    "\n",
    "            test_loss, rmse_test = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
    "            test_loss, rmse_test = test_loss.cpu().data.numpy(), rmse_test.cpu().data.numpy()\n",
    "\n",
    "            print('Epoch: %4d, Train loss: %6.5f Test loss: %6.5f Train RMSE: %.5f Test RMSE: %.5f' %\n",
    "                  (i, loss.cpu().data.numpy() / len(x_train), test_loss / len(x_test), rmse_train, rmse_test))\n",
    "\n",
    "    train_loss, train_rmse = net.get_loss_and_rmse(x_train, y_train, num_samples=num_samples)\n",
    "    test_loss, test_rmse = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
    "\n",
    "    # train_logliks.append((train_loss.cpu().data.numpy() / len(x_train) + np.log(y_stds)[0]))\n",
    "    # test_logliks.append((test_loss.cpu().data.numpy() / len(x_test) + np.log(y_stds)[0]))\n",
    "\n",
    "    train_logliks.append(train_loss.cpu().data.numpy() / len(x_train))\n",
    "    test_logliks.append(test_loss.cpu().data.numpy() / len(x_test))\n",
    "\n",
    "    train_rmses.append(train_rmse.cpu().data.numpy())\n",
    "    test_rmses.append(test_rmse.cpu().data.numpy())\n",
    "\n",
    "    # plt.figure()\n",
    "    # # plot history of accuracy\n",
    "    # # Plot training & validation accuracy values\n",
    "    # plt.plot(history_rmse)\n",
    "    # plt.plot(history_rmse_test)\n",
    "    # plt.title('Model accuracy')\n",
    "    # plt.ylabel('RMSE')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    # plt.show()\n",
    "    #\n",
    "    # plt.figure()\n",
    "    # # Plot training & validation loss values\n",
    "    # plt.plot(history_loss)\n",
    "    # plt.plot(history_loss_test)\n",
    "    # plt.title('Model loss')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    # plt.show()\n",
    "\n",
    "    print('Train log. lik. = %6.5f +/- %6.5f' % (-np.array(train_logliks).mean(), np.array(train_logliks).var() ** 0.5))\n",
    "    print('Test  log. lik. = %6.5f +/- %6.5f' % (-np.array(test_logliks).mean(), np.array(test_logliks).var() ** 0.5))\n",
    "    print('Train RMSE      = %6.5f +/- %6.5f' % (np.array(train_rmses).mean(), np.array(train_rmses).var() ** 0.5))\n",
    "    print('Test  RMSE      = %6.5f +/- %6.5f' % (np.array(test_rmses).mean(), np.array(test_rmses).var() ** 0.5))\n",
    "\n",
    "    return net, x_test, y_test\n",
    "\n",
    "\n",
    "def train_mc_dropout_Kfold(data, drop_prob, n_splits, num_epochs, num_units, learn_rate, weight_decay, log_every,\n",
    "                           num_samples):\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    in_dim = data.shape[1] - 1\n",
    "    train_logliks, test_logliks = [], []\n",
    "    train_rmses, test_rmses = [], []\n",
    "\n",
    "    if plot_history == True:\n",
    "        avg_history_loss, avg_history_loss_test, avg_history_rmse, avg_history_rmse_test = [], [], [], []\n",
    "\n",
    "    # # random shuffle data\n",
    "    # np.random.shuffle(data)\n",
    "\n",
    "    for j, idx in enumerate(kf.split(data)):\n",
    "        \n",
    "        if plot_history == True:\n",
    "            history_loss, history_loss_test, history_rmse, history_rmse_test = [], [], [], []\n",
    "        \n",
    "        print('FOLD %d:' % j)\n",
    "        \n",
    "        train_index, test_index = idx\n",
    "\n",
    "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
    "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
    "        print(x_train.shape, x_test.shape)\n",
    "\n",
    "        # x_means, x_stds = x_train.mean(axis=0), x_train.var(axis=0) ** 0.5\n",
    "        # y_means, y_stds = y_train.mean(axis=0), y_train.var(axis=0) ** 0.5\n",
    "\n",
    "        net = MC_Dropout_Wrapper(\n",
    "            network=MC_Dropout_Model(input_dim=in_dim, output_dim=1, num_units=num_units, drop_prob=drop_prob),\n",
    "            learn_rate=learn_rate, batch_size=batch_size, weight_decay=weight_decay)\n",
    "\n",
    "        # losses = []\n",
    "        # fit_loss_train = np.zeros(num_epochs)\n",
    "\n",
    "        for i in range(num_epochs):\n",
    "            loss = net.fit(x_train, y_train)\n",
    "\n",
    "            if plot_history == True:\n",
    "                # to save loss & rmse at every epoch\n",
    "                train_loss, rmse_train = net.get_loss_and_rmse(x_train, y_train, num_samples=num_samples)\n",
    "                rmse_train = rmse_train.cpu().data.numpy()\n",
    "\n",
    "                test_loss, rmse_test = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
    "                test_loss, rmse_test = test_loss.cpu().data.numpy(), rmse_test.cpu().data.numpy()\n",
    "\n",
    "                history_loss.append(loss.cpu().data.numpy() / len(x_train))\n",
    "                history_loss_test.append(test_loss / len(x_test))\n",
    "                history_rmse.append(rmse_train)\n",
    "                history_rmse_test.append(rmse_test)\n",
    "\n",
    "            if i % log_every == 0 or i == num_epochs - 1:\n",
    "                train_loss, train_rmse = net.get_loss_and_rmse(x_train, y_train, num_samples=num_samples)\n",
    "                train_loss, train_rmse = train_loss.cpu().data.numpy(), train_rmse.cpu().data.numpy()\n",
    "\n",
    "                test_loss, test_rmse = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
    "                test_loss, test_rmse = test_loss.cpu().data.numpy(), test_rmse.cpu().data.numpy()\n",
    "\n",
    "\n",
    "                print('Epoch: %4d, Train loss: %6.5f Test loss: %6.5f Train RMSE: %.5f Test RMSE: %.5f' %\n",
    "                      (i, loss.cpu().data.numpy() / len(x_train), test_loss / len(x_test), train_rmse, test_rmse))\n",
    "        if plot_history == True:\n",
    "            avg_history_loss.append(history_loss)\n",
    "            avg_history_loss_test.append(history_loss_test)\n",
    "            avg_history_rmse.append(history_rmse)\n",
    "            avg_history_rmse_test.append(history_rmse_test)\n",
    "\n",
    "        train_loss, train_rmse = net.get_loss_and_rmse(x_train, y_train, num_samples=num_samples)\n",
    "        test_loss, test_rmse = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
    "\n",
    "        train_logliks.append(train_loss.cpu().data.numpy() / len(x_train) )\n",
    "        test_logliks.append(test_loss.cpu().data.numpy() / len(x_test) )\n",
    "\n",
    "        train_rmses.append(train_rmse.cpu().data.numpy())\n",
    "        test_rmses.append(test_rmse.cpu().data.numpy())\n",
    "\n",
    "    if plot_history == True:\n",
    "        # Save plots\n",
    "        legends = ['Train', 'Test']\n",
    "        format = True  # true for pdf, false for pgf\n",
    "        file_name1 = 'Epoch_vs_Loss'\n",
    "        file_name2 = 'Epoch_vs_RMSE'\n",
    "\n",
    "        avg_history_loss = np.array(avg_history_loss).mean(axis=0)\n",
    "        avg_history_loss_test = np.array(avg_history_loss_test).mean(axis=0)\n",
    "        avg_history_rmse = np.array(avg_history_rmse).mean(axis=0)\n",
    "        avg_history_rmse_test = np.array(avg_history_rmse_test).mean(axis=0)\n",
    "\n",
    "        labels = ['Epoch', 'Loss']\n",
    "\n",
    "        x1, y1 = list(range(1, avg_history_loss.shape[0]+1)), avg_history_loss\n",
    "        x2, y2 = list(range(1, avg_history_loss_test.shape[0] + 1)), avg_history_loss_test\n",
    "        sF.save_plot(x1, y1, labels, legends, format, file_name1, x2, y2)\n",
    "\n",
    "        x3, y3 = list(range(1, avg_history_rmse.shape[0] + 1)), avg_history_rmse\n",
    "        x4, y4 = list(range(1, avg_history_rmse_test.shape[0] + 1)), avg_history_rmse_test\n",
    "        sF.save_plot(x3, y3, labels, legends, format, file_name2, x4, y4)\n",
    "\n",
    "\n",
    "\n",
    "    # labels = ['Epoch', 'RMSE']\n",
    "    # x1, y1 = list(range(1, len(history_rmse)+1)), history_rmse\n",
    "    # x2, y2 = list(range(1, len(history_rmse_test)+1)), history_rmse_test\n",
    "    # sF.save_plot(x1, y1, labels, legends, format, file_name1, x2, y2)\n",
    "    #\n",
    "    # labels = ['Epoch', 'Loss']\n",
    "    # x3, y3 = list(range(1, len(history_loss)+1)), history_loss\n",
    "    # x4, y4 = list(range(1, len(history_loss_test)+1)), history_loss_test\n",
    "    # sF.save_plot(x3, y3, labels, legends, format, file_name2, x4, y4)\n",
    "\n",
    "\n",
    "    print('Train log. lik. = %6.5f +/- %6.5f' % (-np.array(train_logliks).mean(), np.array(train_logliks).var() ** 0.5))\n",
    "    print('Test  log. lik. = %6.5f +/- %6.5f' % (-np.array(test_logliks).mean(), np.array(test_logliks).var() ** 0.5))\n",
    "    print('Train RMSE      = %6.5f +/- %6.5f' % (np.array(train_rmses).mean(), np.array(train_rmses).var() ** 0.5))\n",
    "    print('Test  RMSE      = %6.5f +/- %6.5f' % (np.array(test_rmses).mean(), np.array(test_rmses).var() ** 0.5))\n",
    "\n",
    "    return net, x_test, y_test\n",
    "\n",
    "\n",
    "def main(p_drop, nb_units):\n",
    "\n",
    "#     # # Build the network, K-fold\n",
    "#     net, x_tst, y_tst = train_mc_dropout_Kfold(data=train_data, drop_prob=p_drop, num_epochs=nb_epochs,\n",
    "#                                                n_splits=int(1 / (1 - train_data_ratio)), num_units=nb_units,\n",
    "#                                                learn_rate=learn_rate,\n",
    "#                                                weight_decay=wght_decay, num_samples=10, log_every=100)\n",
    "\n",
    "    # without K-fold\n",
    "    net, x_tst, y_tst = train_mc_dropout(data=train_data, drop_prob=p_drop, num_epochs=nb_epochs,\n",
    "                                               ratio_train_data=train_data_ratio, num_units=nb_units,\n",
    "                                               learn_rate=learn_rate,\n",
    "                                               weight_decay=wght_decay, num_samples=10, log_every=100)\n",
    "\n",
    "\n",
    "    # # print model parameters\n",
    "    # for param in net.network.parameters():\n",
    "    #   print(param.data)\n",
    "\n",
    "    # # save pytorch model\n",
    "    # torch.save(net.network, 'BNN_BLmodel.pt')\n",
    "    # BL_model = torch.load('BNN_BLmodel.pt')\n",
    "    # net.network = BL_model\n",
    "\n",
    "#     # Get a tuple of unique values & their first index location from a numpy array\n",
    "#     uniqueValues, indicesList = np.unique(x_tst[:, 0], return_index=True)\n",
    "#     x_tst = x_tst[indicesList]\n",
    "\n",
    "#     # Only use the selected unique test data\n",
    "#     y_tst = y_tst[indicesList]\n",
    "\n",
    "    # add extra test data\n",
    "    # testdata = normalize([280,35,0.65,3,3], mean_data_x, std_data_x)\n",
    "    # testdata = normalize_max_min([280,35,0.65,3,3], max_x, min_x)\n",
    "    # x_tst = np.vstack((x_tst, testdata)).astype(np.float32)\n",
    "\n",
    "#     # sort test data wrt. 1st column, temperature data\n",
    "#     # Returns the indices that would sort an array.\n",
    "#     sorted_indices = x_tst[:, 0].argsort()\n",
    "#     x_tst = x_tst[sorted_indices]\n",
    "#     y_tst = y_tst[sorted_indices]\n",
    "\n",
    "#     # Find index where elements change value numpy (temperature value first column)\n",
    "#     # sorted_test_data[:-1, 0] != sorted_test_data[1:, 0] or np.where(v[:-1] != v[1:])[0] (for indices)\n",
    "#     # unique_temp_index: indices up to which same temperature data is sorted, every index value is the last data of that\n",
    "#     # part that is printed with that temperature\n",
    "#     unique_temp_index = np.where(x_tst[:-1, 0] != x_tst[1:, 0])[0]\n",
    "\n",
    "#     print(unique_temp_index)\n",
    "    \n",
    "    \n",
    "    x_pred = torch.tensor(x_tst.astype(np.float32))  # convert to torch tensor\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Create an input array that reflects a part, and make overall BL predictions for the leave-one-out part\n",
    "#     #  MODEL BOND LENGTH  \n",
    "    \n",
    "#     # part 25, T=266, v=32, h=0.6\n",
    "#     T, v, h = 266, 32, 0.6\n",
    "    \n",
    "#     # Minimize(abs(pred_mean – target))\n",
    "#     target  = 4.2 # desired part thickness in mm\n",
    "    \n",
    "#     num_layers = np.int(target / h); # number of layers\n",
    "\n",
    "#     num_interfaces = 14     # number of interfaces per layer\n",
    "#     width = 0.8             # filament width in mm\n",
    "\n",
    "#     ycoord = 0.5 * h        # 0.5*height of a layer in mm\n",
    "#     iki_y = ycoord * 2\n",
    "    \n",
    "#     # Create an input array to predict overall part thickness\n",
    "#     inp_BL = [] # input to BNN to make predictions\n",
    "    \n",
    "#     # store inputs for GP(model disrepancy at each interface)\n",
    "#     for jj in range(1, num_layers + 1):\n",
    "#         for ii in range(1, num_interfaces + 1):\n",
    "#             # use x & y coordinates of vertical bonds as training data for the GP\n",
    "#             # Inp =[ Temperature, speed, height, x, y ]\n",
    "#             inp_BL.append([T, v, h, ii * width, ycoord + (jj - 1) * iki_y])\n",
    "\n",
    "#     # Convert built Python lists to a Numpy array.\n",
    "#     inp_BL = np.array(inp_BL, dtype='float32')\n",
    "\n",
    "    \n",
    "# #     x_tst = denormalize_max_min(x_tst, max_x, min_x)\n",
    "# #     print(x_tst.shape)\n",
    "# #     print(x_tst[unique_temp_index[-1],:],unique_temp_index[-1])\n",
    "    \n",
    "    \n",
    "#     # normalize data\n",
    "#     inp_BL = normalize_max_min(inp_BL, max_x, min_x)\n",
    "\n",
    "    \n",
    "    \n",
    "#     x_pred = torch.tensor(inp_BL)  # convert to torch tensor\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    samples = []\n",
    "    noises = []\n",
    "    for i in range(100):\n",
    "#         preds = net.network.forward(x_pred).cpu().data.numpy()\n",
    "        preds = net.network.forward(x_pred.cuda()).cpu().data.numpy()\n",
    "        samples.append(denormalize_max_min(preds[:, 0], max_y, min_y))\n",
    "        noises.append(denormalize_max_min(np.exp(preds[:, 1]), max_y, min_y))\n",
    "\n",
    "    samples, noises = np.array(samples),  np.array(noises)\n",
    "    means = (samples.mean(axis=0)).reshape(-1)\n",
    "\n",
    "    # model precision\n",
    "    # tau = l2 * (1-p_drop) / (2*y.shape[0]*wght_decay)\n",
    "\n",
    "    aleatoric = (noises ** 2).mean(axis=0) ** 0.5\n",
    "    epistemic = (samples.var(axis=0) ** 0.5).reshape(-1)\n",
    "    total_unc = (aleatoric ** 2 + epistemic ** 2) ** 0.5\n",
    "\n",
    "#     print(\"Aleatoric uncertainty mean: {0:.4f}, Epistemic uncertainty mean: {1:.4f}, Total uncertainty mean: {2:.4f}\"\n",
    "#           .format(aleatoric.mean(), epistemic.mean(), total_unc.mean()))\n",
    "#     print(\"Aleatoric uncertainty std: {0:.4f}, Epistemic uncertainty std: {1:.4f}, Total uncertainty std: {2:.4f}\"\n",
    "#           .format(aleatoric.std(), epistemic.std(), total_unc.std()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    mu_bl = means.mean()\n",
    "    sig_bl_al = aleatoric.mean()\n",
    "    sig_bl_ep = epistemic.mean()\n",
    "    sig_bl = total_unc.mean()\n",
    "    \n",
    "    means = [mu_bl] * 3 # 3 number of sets\n",
    "    aleatoric = [sig_bl_al] * 3\n",
    "    epistemic = [sig_bl_ep] * 3\n",
    "    total_unc = [sig_bl] * 3\n",
    "    \n",
    "    # Convert built Python lists to a Numpy array.\n",
    "    means = np.array(means, dtype='float32')\n",
    "    aleatoric = np.array(aleatoric, dtype='float32')\n",
    "    epistemic = np.array(epistemic, dtype='float32')\n",
    "    total_unc = np.array(total_unc, dtype='float32')\n",
    "    \n",
    "    print(means,aleatoric,epistemic,total_unc)\n",
    "    \n",
    "    \n",
    "#     # Dimensionless BL: non-dimensionalize the BL by dividing with the layer height\n",
    "#     dimensionless_mean_bl = means.mean()/height\n",
    "#     dimensionless_total_unc_bl = total_unc.mean()/height**2\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # denormalize test data\n",
    "    x_tst = denormalize_max_min(x_tst, max_x, min_x)\n",
    "    y_tst = denormalize_max_min(y_tst, max_y, min_y)\n",
    "    \n",
    "    # mean bl of all rounds\n",
    "    y_tst_mean_rounds = y_tst.mean(axis=0)\n",
    "    \n",
    "    # mean bl overall of each round of part 25\n",
    "    y_test_overall_each = y_tst.mean(axis=1)\n",
    "    \n",
    "    # mean bl of overall part 25\n",
    "    y_test_overall = y_tst_mean_rounds.mean(axis=0)\n",
    "    \n",
    "    print(y_test_overall, y_test_overall_each)    \n",
    "\n",
    "    \n",
    "    # PLOT FIGURES\n",
    "#     c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
    "#          '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "\n",
    "#     xx = ['Set 1','Set 2','Set 3']\n",
    "    \n",
    "#     # Only plot some portion of the test data\n",
    "#     fig = plt.figure()\n",
    "#     plt.style.use('default')\n",
    "#     plt.plot(xx, y_test_overall_each, 'b.', label='Observations');\n",
    "#     plt.fill_between(xx, means + epistemic, means + total_unc, color=c[9], alpha=0.5,\n",
    "#                      label='Aleatoric')\n",
    "#     plt.fill_between(xx, means - total_unc, means - epistemic, color=c[9], alpha=0.5)\n",
    "#     plt.fill_between(xx, means - epistemic, means + epistemic, color=c[5], alpha=0.6,\n",
    "#                      label='Epistemic')\n",
    "#     plt.fill_between(xx, means + total_unc, means + 2*total_unc, color=c[9], alpha=0.25)\n",
    "#     plt.fill_between(xx, means - total_unc, means - 2*total_unc, color=c[9], alpha=0.25)\n",
    "#     plt.fill_between(xx, means + 2*total_unc, means + 3*total_unc, color=c[9], alpha=0.1)\n",
    "#     plt.fill_between(xx, means - 2*total_unc, means - 3*total_unc, color=c[9], alpha=0.1)\n",
    "#     plt.plot(xx, means, color='black', linewidth=1, label='Predictive mean')\n",
    "#     plt.xlabel('Sets', fontsize=16)\n",
    "#     plt.ylabel('Average bond length (mm)', fontsize=16)\n",
    "#     plt.legend()\n",
    "#     filename='Part25_Predictions'\n",
    "#     fig.savefig(\"{}.pdf\".format(filename), bbox_inches='tight', dpi=300)\n",
    "#     plt.show()\n",
    "    return y_test_overall_each, means, epistemic, total_unc\n",
    "    \n",
    "\n",
    "\n",
    "# Do an exhaustive search if number of units and dropout probabilities are lists\n",
    "if type(nb_units) is list and type(p_drop_rate) is list:\n",
    "    for nb_unit in nb_units:\n",
    "        print('\\n Number of units: ', nb_unit)\n",
    "        for p_drop in p_drop_rate:\n",
    "            print('\\n Dropout prob.: ', p_drop)\n",
    "            y_test_overall_each, means, epistemic, total_unc = main(p_drop, nb_unit)\n",
    "elif type(nb_units) is list:\n",
    "    for nb_unit in nb_units:\n",
    "        print('\\n Number of units: ', nb_unit)\n",
    "        y_test_overall_each, means, epistemic, total_unc = main(p_drop_rate, nb_unit)\n",
    "elif type(p_drop_rate) is list:\n",
    "    for p_drop in p_drop_rate:\n",
    "        print('\\n Dropout prob.: ', p_drop)\n",
    "        y_test_overall_each, means, epistemic, total_unc = main(p_drop, nb_units)\n",
    "else:\n",
    "    y_test_overall_each, means, epistemic, total_unc = main(p_drop_rate, nb_units)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import os\n",
    "\n",
    "# Adjust your matplotlib script by adding the following lines after import matplotlib\n",
    "matplotlib.use(\"pdf\")\n",
    "# matplotlib.use(\"pgf\")\n",
    "    \n",
    "matplotlib.rcParams.update({\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        'font.family': 'serif',\n",
    "        'text.usetex': True,\n",
    "        'pgf.rcfonts': False,\n",
    "    })\n",
    "# add LaTeX on python path\n",
    "user_name = os.getlogin()\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Users/' + user_name + '/AppData/Local/Programs/MiKTeX 2.9/miktex/bin/x64'\n",
    "\n",
    "#===========================     Using LaTeX compatible fonts      =============================== #\n",
    "# use LaTeX fonts in the plot\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')  \n",
    "\n",
    "filename='Part25_Predictions'    \n",
    "labels = ['Sets', 'Average bond length (mm)']\n",
    "\n",
    "c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
    "         '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "\n",
    "# xx = np.array([1,2,3])\n",
    "xx =  np.array(['Set 1', 'Set 2', 'Set 3'])\n",
    "    \n",
    "# get the figure\n",
    "fig = plt.figure()\n",
    "\n",
    "# Only plot some portion of the test data\n",
    "plt.style.use('default')\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "for ii in xx:\n",
    "    if ii == \"Set 3\":\n",
    "        plt.fill_between(xx, means + epistemic, means + total_unc, where=(xx==ii), color=c[9], alpha=0.5,\n",
    "                         label='Aleatoric',lw=5)\n",
    "        plt.fill_between(xx, means - epistemic, means + epistemic, where=(xx==ii), color=c[5], alpha=0.6,\n",
    "                         label='Epistemic',lw=5)\n",
    "        plt.fill_between(xx, means - total_unc, means - epistemic, where=(xx==ii), color=c[9], alpha=0.5,lw=5)\n",
    "        plt.fill_between(xx, means + total_unc, means + 2*total_unc, where=(xx==ii), color=c[9], alpha=0.25,lw=5)\n",
    "        plt.fill_between(xx, means - total_unc, means - 2*total_unc, where=(xx==ii), color=c[9], alpha=0.25,lw=5)\n",
    "        plt.fill_between(xx, means + 2*total_unc, means + 3*total_unc, where=(xx==ii), color=c[9], alpha=0.1,lw=5)\n",
    "        plt.fill_between(xx, means - 2*total_unc, means - 3*total_unc, where=(xx==ii), color=c[9], alpha=0.1,lw=5)\n",
    "    else:\n",
    "        plt.fill_between(xx, means - total_unc, means - epistemic, where=(xx==ii), color=c[9], alpha=0.5,lw=5)\n",
    "        plt.fill_between(xx, means + epistemic, means + total_unc, where=(xx==ii), color=c[9], alpha=0.5,lw=5)\n",
    "        plt.fill_between(xx, means - epistemic, means + epistemic, where=(xx==ii), color=c[5], alpha=0.6,lw=5)\n",
    "        plt.fill_between(xx, means + total_unc, means + 2*total_unc, where=(xx==ii), color=c[9], alpha=0.25,lw=5)\n",
    "        plt.fill_between(xx, means - total_unc, means - 2*total_unc, where=(xx==ii), color=c[9], alpha=0.25,lw=5)\n",
    "        plt.fill_between(xx, means + 2*total_unc, means + 3*total_unc, where=(xx==ii), color=c[9], alpha=0.1,lw=5)\n",
    "        plt.fill_between(xx, means - 2*total_unc, means - 3*total_unc, where=(xx==ii), color=c[9], alpha=0.1,lw=5)\n",
    "\n",
    "plt.plot(xx, y_test_overall_each, 'b.', label='Observations');\n",
    "plt.scatter(xx, means, color='black', marker='x', label='Predictive mean')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(r'{}'.format(labels[0]), fontsize=16)\n",
    "plt.ylabel(r'{}'.format(labels[1]), fontsize=16)\n",
    "plt.tick_params(labelsize=13)\n",
    "plt.legend(fontsize=12) # using a size in points\n",
    "# plt.ylim(bottom=0)  # adjust the bottom leaving top unchanged\n",
    "# plt.xlim(left=0.1)\n",
    "# plt.show()\n",
    "\n",
    "# labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "# labels[0] = 'Set 1'\n",
    "# labels[1] = 'Set 2'\n",
    "# labels[2] = 'Set 3'\n",
    "# labels = ['Set 1', 'Set 2', 'Set 3']\n",
    "# ax.set_xticklabels(labels)\n",
    "\n",
    "# save as PDF\n",
    "fig.savefig(\"{}.pdf\".format(filename), bbox_inches='tight', dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
