{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ### import file from another folder ###\n",
    "# import sys\n",
    "# # insert at 1, 0 is the script path (or '' in REPL)\n",
    "# sys.path.insert(1, '../Python-Save-Plots')\n",
    "# import SaveFigAsPDF_PGF as sF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0, Train loss: 1.06571 Test loss: 258.92633 Train RMSE: 0.37749 Test RMSE: 0.47245\n",
      "Epoch:  100, Train loss: -1.19337 Test loss: -328.85349 Train RMSE: 0.06982 Test RMSE: 0.07141\n",
      "Epoch:  200, Train loss: -1.29386 Test loss: -347.58875 Train RMSE: 0.06443 Test RMSE: 0.07034\n",
      "Epoch:  300, Train loss: -1.36277 Test loss: -369.20682 Train RMSE: 0.06197 Test RMSE: 0.06632\n",
      "Epoch:  400, Train loss: -1.35969 Test loss: -377.73511 Train RMSE: 0.06082 Test RMSE: 0.06455\n",
      "Epoch:  500, Train loss: -1.42940 Test loss: -376.43350 Train RMSE: 0.05963 Test RMSE: 0.06498\n",
      "Epoch:  600, Train loss: -1.44722 Test loss: -377.59814 Train RMSE: 0.05854 Test RMSE: 0.06472\n",
      "Epoch:  700, Train loss: -1.45833 Test loss: -378.46387 Train RMSE: 0.05789 Test RMSE: 0.06548\n",
      "Epoch:  800, Train loss: -1.48486 Test loss: -382.34613 Train RMSE: 0.05700 Test RMSE: 0.06447\n",
      "Epoch:  900, Train loss: -1.49728 Test loss: -386.57703 Train RMSE: 0.05623 Test RMSE: 0.06370\n",
      "Epoch: 1000, Train loss: -1.50876 Test loss: -383.03729 Train RMSE: 0.05555 Test RMSE: 0.06447\n",
      "Epoch: 1100, Train loss: -1.51327 Test loss: -386.81873 Train RMSE: 0.05487 Test RMSE: 0.06372\n",
      "Epoch: 1200, Train loss: -1.52182 Test loss: -382.55179 Train RMSE: 0.05443 Test RMSE: 0.06471\n",
      "Epoch: 1300, Train loss: -1.52784 Test loss: -386.31973 Train RMSE: 0.05395 Test RMSE: 0.06409\n",
      "Epoch: 1400, Train loss: -1.54020 Test loss: -386.82315 Train RMSE: 0.05412 Test RMSE: 0.06469\n",
      "Epoch: 1499, Train loss: -1.54501 Test loss: -388.79959 Train RMSE: 0.05326 Test RMSE: 0.06389\n",
      "Train log. lik. = 1.59385 +/- 0.00000\n",
      "Test  log. lik. = 389.01215 +/- 0.00000\n",
      "Train RMSE      = 0.05355 +/- 0.00000\n",
      "Test  RMSE      = 0.06369 +/- 0.00000\n",
      "[0.5191024 0.5191024 0.5191024] [0.04974113 0.04974113 0.04974113] [0.01372799 0.01372799 0.01372799] [0.05165344 0.05165344 0.05165344]\n",
      "[0.52408457] [[0.52455556]\n",
      " [0.5266612 ]\n",
      " [0.5210369 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters\n",
    "wght_decay, learn_rate = 0.1, 4e-3\n",
    "train_data_ratio, nb_epochs = 0.8, 1500\n",
    "p_drop_rate = 0.1\n",
    "nb_units = 100\n",
    "# nb_units = [10, 20, 30, 50, 100]\n",
    "# p_drop_rate = [0.05, 0.075, 0.1, 0.15, 0.2, 0.3, 0.4]\n",
    "\n",
    "plot_history = False # plot loss and rmse vs epochs if True\n",
    "\n",
    "def normalize_max_min(data, data_max, data_min):\n",
    "    return (data - data_min) / (data_max - data_min)\n",
    "\n",
    "def denormalize_max_min(data, data_max, data_min):\n",
    "    return data * (data_max - data_min) + data_min\n",
    "\n",
    "def measurements_and_training_data(num_parts=25, ratio_=1.0):\n",
    "    \"\"\"\n",
    "    num_parts: is the number of parts to be analyzed, ratio_: is the ratio of the data to be used as training\n",
    "    This function load the dataset for the bond length of each part (1 to num_parts)\n",
    "    \"\"\"\n",
    "    # =================================================================================================================== %\n",
    "    # _______________________________________________       INPUTS     ___________________________________________________\n",
    "    # =================================================================================================================== %\n",
    "    wth = 0.8  # width of filaments [m]\n",
    "\n",
    "    # =================================================================================================================== %\n",
    "    # ___________________________________        IMPORT Measured BOND LENGTH DATA     ____________________________________\n",
    "    # =================================================================================================================== %\n",
    "    # Directory where measurements are stored\n",
    "    dir_measurements = \"./data/sets\"\n",
    "\n",
    "    # total number of sets\n",
    "    total_sets = 3\n",
    "\n",
    "    # IMPORT Measured BOND LENGTH DATA\n",
    "    start_part_id = 1  # starting ID part\n",
    "\n",
    "    total_parts = np.arange(start_part_id, num_parts + 1, dtype='int32')\n",
    "    index_to_remove = [13, 20]  # remove parts that do not have measurements, i.e., 5,14,21\n",
    "    total_parts = np.delete(total_parts, index_to_remove)  # remove elements\n",
    "\n",
    "    ctr = 0  # counter of parts\n",
    "    measured_bl_row = []  # list stores bond length (BL) data of all parts\n",
    "    inp = []  # list storing training data inputs\n",
    "\n",
    "    # measured_bl_sets = [] # list of BL sets\n",
    "\n",
    "    for set_id in range(1, total_sets + 1):  # loop over each part set\n",
    "        measured_bl_eachSet = []\n",
    "        for part_id in total_parts:  # loop over each manufactured part ID\n",
    "\n",
    "            # Read Process parameters & bond length measurements\n",
    "            bl_file = dir_measurements + \"/Round\" + str(set_id) + \"/PPSet\" + str(part_id) + \".xlsx\"\n",
    "\n",
    "            df = pd.read_excel(bl_file, header=None)  # read the excel file\n",
    "\n",
    "            num_layers = df.iloc[2, 0]  # # of layers\n",
    "            num_interfaces = df.iloc[-2, 1]  # # of interfaces/layer\n",
    "            num_interfaces_of_a_part = int(num_layers * num_interfaces)  # num. of interfaces of that part\n",
    "\n",
    "            # save printer temperature, speed, height input for each part\n",
    "            t_n = df.iloc[0, 1]  # Printer nozzle temperature(ยบC)\n",
    "            v_p = df.iloc[0, 3]  # printer speed mm / s\n",
    "            hth = df.iloc[0, 5]  # layer height[mm]\n",
    "            t_n = float(t_n)\n",
    "            v_p = float(v_p)\n",
    "\n",
    "            # print('T_N, v_p, height:', t_n, v_p, hth, \"\\n\")\n",
    "\n",
    "            raw_measuredbl = df.iloc[2:-1, 3]  # measured bond lengths between each interface\n",
    "            raw_measuredbl = raw_measuredbl.astype('float32')\n",
    "\n",
    "            # reshape the measured bond length array & convert to numpy ndarray\n",
    "            reshaped_measured_bl = raw_measuredbl.values.reshape(num_interfaces, num_layers, order='F').copy()\n",
    "\n",
    "            # first column is 1st layer and soon(each row is each interface bond length, BL)\n",
    "            measured_bl = np.fliplr(reshaped_measured_bl).T  # flip matrix left to right\n",
    "\n",
    "            # store measured BL data of all parts in order reshaped in row\n",
    "            measured_bl_row.append([measured_bl.reshape(num_interfaces_of_a_part).copy()])\n",
    "            measured_bl_eachSet.append([measured_bl.reshape(num_interfaces_of_a_part).copy()])\n",
    "\n",
    "            # if part_id==5:\n",
    "            #     print(measured_bl.T)\n",
    "\n",
    "            # Prediction inputs are x & y coordinates of vertical bond length locations\n",
    "            # x, y coordinate of layer 1 & interface 1(vertical)\n",
    "            ycoord = 0.5 * hth  # 0.5*height of a layer in mm\n",
    "            iki_y = ycoord * 2\n",
    "\n",
    "            # store inputs for GP(model disrepancy at each interface)\n",
    "            for jj in range(1, num_layers + 1):\n",
    "                for ii in range(1, num_interfaces + 1):\n",
    "                    # use x & y coordinates of vertical bonds as training data for the GP\n",
    "                    # Inp =[ Temperature, speed, height, x, y ]\n",
    "                    inp.append([t_n, v_p, hth, ii * wth, ycoord + (jj - 1) * iki_y])\n",
    "\n",
    "            ctr += 1  # increment counter to keep track of total number of parts analyzed\n",
    "\n",
    "    #     # to save 3 sets of data on 3 columns to analyze mean and std of data for aleatoric\n",
    "    #     measured_bl_sets.append(measured_bl_eachSet)\n",
    "    # measured_bl_set1 = np.concatenate(measured_bl_sets[0], axis=1)\n",
    "    # measured_bl_set1 = measured_bl_set1.T  # transpose s.t. the number of rows matches Inp\n",
    "    # measured_bl_set2 = np.concatenate(measured_bl_sets[1], axis=1)\n",
    "    # measured_bl_set2 = measured_bl_set2.T  # transpose s.t. the number of rows matches Inp\n",
    "    # measured_bl_set3 = np.concatenate(measured_bl_sets[2], axis=1)\n",
    "    # measured_bl_set3 = measured_bl_set3.T  # transpose s.t. the number of rows matches Inp\n",
    "    # measured_bl_setsAll = np.hstack((measured_bl_set1,measured_bl_set2,measured_bl_set3))\n",
    "\n",
    "    # Inp: stored inputs for Gaussian process\n",
    "    # (model disrepancy at each interface): [T_N, v_p, hth, x, y]\n",
    "    # Convert built Python lists to a Numpy array.\n",
    "    inp = np.array(inp, dtype='float32')\n",
    "\n",
    "    # concatenating different size arrays stored in a list\n",
    "    measured_bl_row = np.concatenate(measured_bl_row, axis=1)\n",
    "    measured_bl_row = measured_bl_row.T  # transpose s.t. the number of rows matches Inp\n",
    "\n",
    "    # Normalize training data\n",
    "    # inp = (inp - inp.mean(axis=0)) / inp.std(axis=0)  # take mean of each column\n",
    "    # measured_bl_row = (measured_bl_row - measured_bl_row.mean(axis=0)) / measured_bl_row.std(axis=0)  # take mean of each column\n",
    "\n",
    "    alldata = np.hstack((inp, measured_bl_row))  # stack 2 numpy arrays column-wise\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    #               Random Permutation of Training Data\n",
    "    # -------------------------------------------------------------------------\n",
    "    nl = inp.shape[0]  # size of training data\n",
    "\n",
    "    # randomly select RatioToBeUsed to be training set for GP model\n",
    "    num_train = round(ratio_ * nl)\n",
    "    idx_ = np.random.permutation(nl)\n",
    "    # idx_ = np.arange(nl)  # do not do random permutation\n",
    "\n",
    "    # Use the first RatioToBeUsed to train the model\n",
    "    idx_train = idx_[0:num_train]\n",
    "    # all_data_train = alldata[idx_train, :]\n",
    "\n",
    "    # mean_of_data = all_data_train.mean(axis=0)\n",
    "    # std_of_data = all_data_train.std(axis=0)\n",
    "    # all_data_train = (all_data_train - mean_of_data) / std_of_data  # take mean of each column\n",
    "\n",
    "    # The (1-RatioToBeUsed) will be used to test the model\n",
    "    idx_test = idx_[(num_train + 1):]\n",
    "    x_test = alldata[idx_test, :-1]\n",
    "    y_test = alldata[idx_test, -1]\n",
    "\n",
    "    return alldata, x_test, y_test\n",
    "\n",
    "# load training and test data\n",
    "train_data, x_tst, y_tst = measurements_and_training_data()\n",
    "\n",
    "x = train_data[:, :-1]  # training data, for all but last column\n",
    "y = train_data[:, -1]  # measurements of the training data, last column\n",
    "\n",
    "# Replace the values of  bond length (BL) that are less than 0.1 with 0.\n",
    "# The reason for this is that the BL values were supposed to be zero,\n",
    "# for those interfaces but not recorded so.\n",
    "y[y < 0.1] = 0\n",
    "\n",
    "# save max and min values of x and y\n",
    "max_x, min_x, max_y, min_y = x.max(axis=0), x.min(axis=0), y.max(axis=0), y.min(axis=0)\n",
    "\n",
    "# import pickle\n",
    "# with open('maxmin.pickle', 'wb') as f:\n",
    "#     pickle.dump([max_x, min_x, max_y, min_y], f)\n",
    "\n",
    "# normalize data\n",
    "x = normalize_max_min(x, max_x, min_x)\n",
    "y = normalize_max_min(y, max_y, min_y)\n",
    "\n",
    "# stack input and output data\n",
    "train_data = np.column_stack((x, y))\n",
    "# batch_size = train_data.shape[0]\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "\n",
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "\n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "\n",
    "def log_gaussian_loss(output, target, sigma, no_dim):\n",
    "    exponent = -0.5 * (target - output) ** 2 / sigma ** 2\n",
    "    log_coeff = -no_dim * torch.log(sigma) - 0.5 * no_dim * np.log(2 * np.pi)\n",
    "    return - (log_coeff + exponent).sum()\n",
    "\n",
    "\n",
    "class MC_Dropout_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_units, drop_prob):\n",
    "        super(MC_Dropout_Model, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        # network with two hidden and one output layer\n",
    "        self.layer1 = nn.Linear(input_dim, num_units)\n",
    "        self.layer2 = nn.Linear(num_units, num_units)\n",
    "        # self.layer3 = nn.Linear(num_units, num_units)\n",
    "        # self.layer4 = nn.Linear(num_units, num_units)\n",
    "        self.layer3 = nn.Linear(num_units, 2 * output_dim)\n",
    "\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = F.dropout(x, p=self.drop_prob, training=True)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.activation(x)\n",
    "        x = F.dropout(x, p=self.drop_prob, training=True)\n",
    "        #\n",
    "        # x = self.layer3(x)\n",
    "        # x = self.activation(x)\n",
    "        # x = F.dropout(x, p=self.drop_prob, training=True)\n",
    "        #\n",
    "        # x = self.layer4(x)\n",
    "        # x = self.activation(x)\n",
    "        # x = F.dropout(x, p=self.drop_prob, training=True)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MC_Dropout_Wrapper:\n",
    "    def __init__(self, network, learn_rate, batch_size, weight_decay):\n",
    "        self.learn_rate = learn_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.network = network\n",
    "        self.network.cuda()\n",
    "\n",
    "        # self.optimizer = torch.optim.SGD(self.network.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "        self.loss_func = log_gaussian_loss\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        x, y = to_variable(var=(x, y), cuda=True)\n",
    "\n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        output = self.network(x)\n",
    "\n",
    "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_loss_and_rmse(self, x, y, num_samples):\n",
    "        x, y = to_variable(var=(x, y), cuda=True)\n",
    "\n",
    "        means, stds = [], []\n",
    "        for i in range(num_samples):\n",
    "            output = self.network(x)\n",
    "            means.append(output[:, :1])\n",
    "            stds.append(output[:, 1:].exp())\n",
    "\n",
    "        means, stds = torch.cat(means, dim=1), torch.cat(stds, dim=1)\n",
    "        mean = means.mean(dim=-1)[:, None]\n",
    "        std = ((means.var(dim=-1) + stds.mean(dim=-1) ** 2) ** 0.5)[:, None]\n",
    "        loss = self.loss_func(mean, y, std, 1)\n",
    "\n",
    "        rmse = ((mean - y) ** 2).mean() ** 0.5\n",
    "\n",
    "        return loss.detach().cpu(), rmse.detach().cpu()\n",
    "\n",
    "\n",
    "def train_mc_dropout(data, drop_prob, ratio_train_data, num_epochs, num_units, learn_rate, weight_decay, log_every,\n",
    "                     num_samples):\n",
    "    in_dim = data.shape[1] - 1\n",
    "    train_logliks, test_logliks = [], []\n",
    "    train_rmses, test_rmses = [], []\n",
    "\n",
    "    # history_loss, history_loss_test, history_rmse, history_rmse_test = [], [], [], []\n",
    "\n",
    "#     # -------------------------------------------------------------------------\n",
    "#     #               Random Permutation of Training Data\n",
    "#     # -------------------------------------------------------------------------\n",
    "#     nl = data.shape[0]  # size of training data\n",
    "\n",
    "#     # randomly select RatioToBeUsed to be training set for GP model\n",
    "#     num_train = round(ratio_train_data * nl)\n",
    "#     idx_ = np.random.permutation(nl)\n",
    "#     # idx_ = np.arange(nl)  # do not do random permutation\n",
    "\n",
    "#     # Use the first RatioToBeUsed to train the model\n",
    "#     idx_train = idx_[0:num_train]\n",
    "\n",
    "#     x_train, y_train = data[idx_train, :in_dim], data[idx_train, in_dim:]\n",
    "\n",
    "#     # The (1-RatioToBeUsed) will be used to test the model\n",
    "#     idx_test = idx_[(num_train + 1):]\n",
    "#     x_test, y_test = data[idx_test, :in_dim], data[idx_test, in_dim:]\n",
    "\n",
    "#     print(x_train.shape, x_test.shape)\n",
    "\n",
    "    x, y = data[:, :in_dim], data[:, in_dim:]\n",
    "\n",
    "    # Find index where elements change value numpy (temperature value first column)\n",
    "    # sorted_test_data[:-1, 0] != sorted_test_data[1:, 0] or np.where(v[:-1] != v[1:])[0] (for indices)\n",
    "    # unique_temp_index: indices up to which same temperature data is sorted, every index value is the last data of that\n",
    "    # part that is printed with that temperature\n",
    "    unique_temp_index = np.where(x[:-1, 0] != x[1:, 0])[0] + 1\n",
    "    unique_temp_index = np.insert(unique_temp_index,0,0)\n",
    "    \n",
    "    x_denorm = denormalize_max_min(x, max_x, min_x)\n",
    "#     print(x_train_denorm[unique_temp_index[:],:],unique_temp_index[-1])\n",
    "    \n",
    "    T = 266 # temperature of the part to be used for leave-one-out\n",
    "#     print(x_train_denorm[unique_temp_index[:],0] == T)\n",
    "#     print(unique_temp_index[x_train_denorm[unique_temp_index[:],0] == T])\n",
    "#     print(unique_temp_index[:])\n",
    "    # using enumerate() + list comprehension \n",
    "    # to return true indices.\n",
    "    test_list = x_denorm[unique_temp_index[:],0] == T\n",
    "    res = [i for i, val in enumerate(test_list) if val]\n",
    "    \n",
    "    start_index = unique_temp_index[res]\n",
    "    # using list comprehension, adding 1 to each element except last one\n",
    "    res = [k + 1 for k in res[:-1]]\n",
    "    end_index = unique_temp_index[res]\n",
    "    end_index = np.append(end_index, x.shape[0])\n",
    "    \n",
    "#     print(res, start_index, end_index)\n",
    "\n",
    "    x_test, y_test = [], []\n",
    "    indices_to_remove=[]\n",
    "    x_test.append(data[start_index[0]:end_index[0], :in_dim])\n",
    "    for ii in range(len(start_index)):\n",
    "        y_test.append(data[start_index[ii]:end_index[ii], in_dim:])\n",
    "#         data_new = np.delete(data,np.s_[start_index[ii]:end_index[ii]],axis=0)\n",
    "        indices_to_remove.append([range(start_index[ii],end_index[ii])])\n",
    "    \n",
    "    # Convert built Python lists to a Numpy array.\n",
    "    x_test = np.array(x_test, dtype='float32')\n",
    "    y_test = np.array(y_test, dtype='float32')\n",
    "\n",
    "    \n",
    "        \n",
    "    data_new = np.delete(data,np.s_[indices_to_remove],axis=0)\n",
    "    \n",
    "    x_train, y_train = data_new[:, :in_dim], data_new[:, in_dim:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    net = MC_Dropout_Wrapper(\n",
    "        network=MC_Dropout_Model(input_dim=in_dim, output_dim=1, num_units=num_units, drop_prob=drop_prob),\n",
    "        learn_rate=learn_rate, batch_size=batch_size, weight_decay=weight_decay)\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        loss = net.fit(x_train, y_train)\n",
    "\n",
    "        # train_loss, rmse_train = net.get_loss_and_rmse(x_train, y_train, num_samples=num_samples)\n",
    "        # rmse_train = rmse_train.cpu().data.numpy()\n",
    "        #\n",
    "        # test_loss, rmse_test = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
    "        # test_loss, rmse_test = test_loss.cpu().data.numpy(), rmse_test.cpu().data.numpy()\n",
    "\n",
    "        # history_loss.append(loss.cpu().data.numpy() / len(x_train))\n",
    "        # history_loss_test.append(test_loss / len(x_test))\n",
    "        # history_rmse.append(rmse_train)\n",
    "        # history_rmse_test.append(rmse_test)\n",
    "\n",
    "        if i % log_every == 0 or i == num_epochs - 1:\n",
    "            train_loss, rmse_train = net.get_loss_and_rmse(x_train, y_train, num_samples=num_samples)\n",
    "            rmse_train = rmse_train.cpu().data.numpy()\n",
    "\n",
    "            test_loss, rmse_test = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
    "            test_loss, rmse_test = test_loss.cpu().data.numpy(), rmse_test.cpu().data.numpy()\n",
    "\n",
    "            print('Epoch: %4d, Train loss: %6.5f Test loss: %6.5f Train RMSE: %.5f Test RMSE: %.5f' %\n",
    "                  (i, loss.cpu().data.numpy() / len(x_train), test_loss / len(x_test), rmse_train, rmse_test))\n",
    "\n",
    "    train_loss, train_rmse = net.get_loss_and_rmse(x_train, y_train, num_samples=num_samples)\n",
    "    test_loss, test_rmse = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
    "\n",
    "    # train_logliks.append((train_loss.cpu().data.numpy() / len(x_train) + np.log(y_stds)[0]))\n",
    "    # test_logliks.append((test_loss.cpu().data.numpy() / len(x_test) + np.log(y_stds)[0]))\n",
    "\n",
    "    train_logliks.append(train_loss.cpu().data.numpy() / len(x_train))\n",
    "    test_logliks.append(test_loss.cpu().data.numpy() / len(x_test))\n",
    "\n",
    "    train_rmses.append(train_rmse.cpu().data.numpy())\n",
    "    test_rmses.append(test_rmse.cpu().data.numpy())\n",
    "\n",
    "    # plt.figure()\n",
    "    # # plot history of accuracy\n",
    "    # # Plot training & validation accuracy values\n",
    "    # plt.plot(history_rmse)\n",
    "    # plt.plot(history_rmse_test)\n",
    "    # plt.title('Model accuracy')\n",
    "    # plt.ylabel('RMSE')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    # plt.show()\n",
    "    #\n",
    "    # plt.figure()\n",
    "    # # Plot training & validation loss values\n",
    "    # plt.plot(history_loss)\n",
    "    # plt.plot(history_loss_test)\n",
    "    # plt.title('Model loss')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    # plt.show()\n",
    "\n",
    "    print('Train log. lik. = %6.5f +/- %6.5f' % (-np.array(train_logliks).mean(), np.array(train_logliks).var() ** 0.5))\n",
    "    print('Test  log. lik. = %6.5f +/- %6.5f' % (-np.array(test_logliks).mean(), np.array(test_logliks).var() ** 0.5))\n",
    "    print('Train RMSE      = %6.5f +/- %6.5f' % (np.array(train_rmses).mean(), np.array(train_rmses).var() ** 0.5))\n",
    "    print('Test  RMSE      = %6.5f +/- %6.5f' % (np.array(test_rmses).mean(), np.array(test_rmses).var() ** 0.5))\n",
    "\n",
    "    return net, x_test, y_test\n",
    "\n",
    "\n",
    "def train_mc_dropout_Kfold(data, drop_prob, n_splits, num_epochs, num_units, learn_rate, weight_decay, log_every,\n",
    "                           num_samples):\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    in_dim = data.shape[1] - 1\n",
    "    train_logliks, test_logliks = [], []\n",
    "    train_rmses, test_rmses = [], []\n",
    "\n",
    "    if plot_history == True:\n",
    "        avg_history_loss, avg_history_loss_test, avg_history_rmse, avg_history_rmse_test = [], [], [], []\n",
    "\n",
    "    # # random shuffle data\n",
    "    # np.random.shuffle(data)\n",
    "\n",
    "    for j, idx in enumerate(kf.split(data)):\n",
    "        \n",
    "        if plot_history == True:\n",
    "            history_loss, history_loss_test, history_rmse, history_rmse_test = [], [], [], []\n",
    "        \n",
    "        print('FOLD %d:' % j)\n",
    "        \n",
    "        train_index, test_index = idx\n",
    "\n",
    "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
    "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
    "        print(x_train.shape, x_test.shape)\n",
    "\n",
    "        # x_means, x_stds = x_train.mean(axis=0), x_train.var(axis=0) ** 0.5\n",
    "        # y_means, y_stds = y_train.mean(axis=0), y_train.var(axis=0) ** 0.5\n",
    "\n",
    "        net = MC_Dropout_Wrapper(\n",
    "            network=MC_Dropout_Model(input_dim=in_dim, output_dim=1, num_units=num_units, drop_prob=drop_prob),\n",
    "            learn_rate=learn_rate, batch_size=batch_size, weight_decay=weight_decay)\n",
    "\n",
    "        # losses = []\n",
    "        # fit_loss_train = np.zeros(num_epochs)\n",
    "\n",
    "        for i in range(num_epochs):\n",
    "            loss = net.fit(x_train, y_train)\n",
    "\n",
    "            if plot_history == True:\n",
    "                # to save loss & rmse at every epoch\n",
    "                train_loss, rmse_train = net.get_loss_and_rmse(x_train, y_train, num_samples=num_samples)\n",
    "                rmse_train = rmse_train.cpu().data.numpy()\n",
    "\n",
    "                test_loss, rmse_test = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
    "                test_loss, rmse_test = test_loss.cpu().data.numpy(), rmse_test.cpu().data.numpy()\n",
    "\n",
    "                history_loss.append(loss.cpu().data.numpy() / len(x_train))\n",
    "                history_loss_test.append(test_loss / len(x_test))\n",
    "                history_rmse.append(rmse_train)\n",
    "                history_rmse_test.append(rmse_test)\n",
    "\n",
    "            if i % log_every == 0 or i == num_epochs - 1:\n",
    "                train_loss, train_rmse = net.get_loss_and_rmse(x_train, y_train, num_samples=num_samples)\n",
    "                train_loss, train_rmse = train_loss.cpu().data.numpy(), train_rmse.cpu().data.numpy()\n",
    "\n",
    "                test_loss, test_rmse = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
    "                test_loss, test_rmse = test_loss.cpu().data.numpy(), test_rmse.cpu().data.numpy()\n",
    "\n",
    "\n",
    "                print('Epoch: %4d, Train loss: %6.5f Test loss: %6.5f Train RMSE: %.5f Test RMSE: %.5f' %\n",
    "                      (i, loss.cpu().data.numpy() / len(x_train), test_loss / len(x_test), train_rmse, test_rmse))\n",
    "        if plot_history == True:\n",
    "            avg_history_loss.append(history_loss)\n",
    "            avg_history_loss_test.append(history_loss_test)\n",
    "            avg_history_rmse.append(history_rmse)\n",
    "            avg_history_rmse_test.append(history_rmse_test)\n",
    "\n",
    "        train_loss, train_rmse = net.get_loss_and_rmse(x_train, y_train, num_samples=num_samples)\n",
    "        test_loss, test_rmse = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
    "\n",
    "        train_logliks.append(train_loss.cpu().data.numpy() / len(x_train) )\n",
    "        test_logliks.append(test_loss.cpu().data.numpy() / len(x_test) )\n",
    "\n",
    "        train_rmses.append(train_rmse.cpu().data.numpy())\n",
    "        test_rmses.append(test_rmse.cpu().data.numpy())\n",
    "\n",
    "    if plot_history == True:\n",
    "        # Save plots\n",
    "        legends = ['Train', 'Test']\n",
    "        format = True  # true for pdf, false for pgf\n",
    "        file_name1 = 'Epoch_vs_Loss'\n",
    "        file_name2 = 'Epoch_vs_RMSE'\n",
    "\n",
    "        avg_history_loss = np.array(avg_history_loss).mean(axis=0)\n",
    "        avg_history_loss_test = np.array(avg_history_loss_test).mean(axis=0)\n",
    "        avg_history_rmse = np.array(avg_history_rmse).mean(axis=0)\n",
    "        avg_history_rmse_test = np.array(avg_history_rmse_test).mean(axis=0)\n",
    "\n",
    "        labels = ['Epoch', 'Loss']\n",
    "\n",
    "        x1, y1 = list(range(1, avg_history_loss.shape[0]+1)), avg_history_loss\n",
    "        x2, y2 = list(range(1, avg_history_loss_test.shape[0] + 1)), avg_history_loss_test\n",
    "        sF.save_plot(x1, y1, labels, legends, format, file_name1, x2, y2)\n",
    "\n",
    "        x3, y3 = list(range(1, avg_history_rmse.shape[0] + 1)), avg_history_rmse\n",
    "        x4, y4 = list(range(1, avg_history_rmse_test.shape[0] + 1)), avg_history_rmse_test\n",
    "        sF.save_plot(x3, y3, labels, legends, format, file_name2, x4, y4)\n",
    "\n",
    "\n",
    "\n",
    "    # labels = ['Epoch', 'RMSE']\n",
    "    # x1, y1 = list(range(1, len(history_rmse)+1)), history_rmse\n",
    "    # x2, y2 = list(range(1, len(history_rmse_test)+1)), history_rmse_test\n",
    "    # sF.save_plot(x1, y1, labels, legends, format, file_name1, x2, y2)\n",
    "    #\n",
    "    # labels = ['Epoch', 'Loss']\n",
    "    # x3, y3 = list(range(1, len(history_loss)+1)), history_loss\n",
    "    # x4, y4 = list(range(1, len(history_loss_test)+1)), history_loss_test\n",
    "    # sF.save_plot(x3, y3, labels, legends, format, file_name2, x4, y4)\n",
    "\n",
    "\n",
    "    print('Train log. lik. = %6.5f +/- %6.5f' % (-np.array(train_logliks).mean(), np.array(train_logliks).var() ** 0.5))\n",
    "    print('Test  log. lik. = %6.5f +/- %6.5f' % (-np.array(test_logliks).mean(), np.array(test_logliks).var() ** 0.5))\n",
    "    print('Train RMSE      = %6.5f +/- %6.5f' % (np.array(train_rmses).mean(), np.array(train_rmses).var() ** 0.5))\n",
    "    print('Test  RMSE      = %6.5f +/- %6.5f' % (np.array(test_rmses).mean(), np.array(test_rmses).var() ** 0.5))\n",
    "\n",
    "    return net, x_test, y_test\n",
    "\n",
    "\n",
    "def main(p_drop, nb_units):\n",
    "\n",
    "#     # # Build the network, K-fold\n",
    "#     net, x_tst, y_tst = train_mc_dropout_Kfold(data=train_data, drop_prob=p_drop, num_epochs=nb_epochs,\n",
    "#                                                n_splits=int(1 / (1 - train_data_ratio)), num_units=nb_units,\n",
    "#                                                learn_rate=learn_rate,\n",
    "#                                                weight_decay=wght_decay, num_samples=10, log_every=100)\n",
    "\n",
    "    # without K-fold\n",
    "    net, x_tst, y_tst = train_mc_dropout(data=train_data, drop_prob=p_drop, num_epochs=nb_epochs,\n",
    "                                               ratio_train_data=train_data_ratio, num_units=nb_units,\n",
    "                                               learn_rate=learn_rate,\n",
    "                                               weight_decay=wght_decay, num_samples=10, log_every=100)\n",
    "\n",
    "\n",
    "    # # print model parameters\n",
    "    # for param in net.network.parameters():\n",
    "    #   print(param.data)\n",
    "\n",
    "    # # save pytorch model\n",
    "    # torch.save(net.network, 'BNN_BLmodel.pt')\n",
    "    # BL_model = torch.load('BNN_BLmodel.pt')\n",
    "    # net.network = BL_model\n",
    "\n",
    "#     # Get a tuple of unique values & their first index location from a numpy array\n",
    "#     uniqueValues, indicesList = np.unique(x_tst[:, 0], return_index=True)\n",
    "#     x_tst = x_tst[indicesList]\n",
    "\n",
    "#     # Only use the selected unique test data\n",
    "#     y_tst = y_tst[indicesList]\n",
    "\n",
    "    # add extra test data\n",
    "    # testdata = normalize([280,35,0.65,3,3], mean_data_x, std_data_x)\n",
    "    # testdata = normalize_max_min([280,35,0.65,3,3], max_x, min_x)\n",
    "    # x_tst = np.vstack((x_tst, testdata)).astype(np.float32)\n",
    "\n",
    "#     # sort test data wrt. 1st column, temperature data\n",
    "#     # Returns the indices that would sort an array.\n",
    "#     sorted_indices = x_tst[:, 0].argsort()\n",
    "#     x_tst = x_tst[sorted_indices]\n",
    "#     y_tst = y_tst[sorted_indices]\n",
    "\n",
    "#     # Find index where elements change value numpy (temperature value first column)\n",
    "#     # sorted_test_data[:-1, 0] != sorted_test_data[1:, 0] or np.where(v[:-1] != v[1:])[0] (for indices)\n",
    "#     # unique_temp_index: indices up to which same temperature data is sorted, every index value is the last data of that\n",
    "#     # part that is printed with that temperature\n",
    "#     unique_temp_index = np.where(x_tst[:-1, 0] != x_tst[1:, 0])[0]\n",
    "\n",
    "#     print(unique_temp_index)\n",
    "    \n",
    "    \n",
    "    x_pred = torch.tensor(x_tst.astype(np.float32))  # convert to torch tensor\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Create an input array that reflects a part, and make overall BL predictions for the leave-one-out part\n",
    "#     #  MODEL BOND LENGTH  \n",
    "    \n",
    "#     # part 25, T=266, v=32, h=0.6\n",
    "#     T, v, h = 266, 32, 0.6\n",
    "    \n",
    "#     # Minimize(abs(pred_mean – target))\n",
    "#     target  = 4.2 # desired part thickness in mm\n",
    "    \n",
    "#     num_layers = np.int(target / h); # number of layers\n",
    "\n",
    "#     num_interfaces = 14     # number of interfaces per layer\n",
    "#     width = 0.8             # filament width in mm\n",
    "\n",
    "#     ycoord = 0.5 * h        # 0.5*height of a layer in mm\n",
    "#     iki_y = ycoord * 2\n",
    "    \n",
    "#     # Create an input array to predict overall part thickness\n",
    "#     inp_BL = [] # input to BNN to make predictions\n",
    "    \n",
    "#     # store inputs for GP(model disrepancy at each interface)\n",
    "#     for jj in range(1, num_layers + 1):\n",
    "#         for ii in range(1, num_interfaces + 1):\n",
    "#             # use x & y coordinates of vertical bonds as training data for the GP\n",
    "#             # Inp =[ Temperature, speed, height, x, y ]\n",
    "#             inp_BL.append([T, v, h, ii * width, ycoord + (jj - 1) * iki_y])\n",
    "\n",
    "#     # Convert built Python lists to a Numpy array.\n",
    "#     inp_BL = np.array(inp_BL, dtype='float32')\n",
    "\n",
    "    \n",
    "# #     x_tst = denormalize_max_min(x_tst, max_x, min_x)\n",
    "# #     print(x_tst.shape)\n",
    "# #     print(x_tst[unique_temp_index[-1],:],unique_temp_index[-1])\n",
    "    \n",
    "    \n",
    "#     # normalize data\n",
    "#     inp_BL = normalize_max_min(inp_BL, max_x, min_x)\n",
    "\n",
    "    \n",
    "    \n",
    "#     x_pred = torch.tensor(inp_BL)  # convert to torch tensor\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    samples = []\n",
    "    noises = []\n",
    "    for i in range(100):\n",
    "#         preds = net.network.forward(x_pred).cpu().data.numpy()\n",
    "        preds = net.network.forward(x_pred.cuda()).cpu().data.numpy()\n",
    "        samples.append(denormalize_max_min(preds[:, 0], max_y, min_y))\n",
    "        noises.append(denormalize_max_min(np.exp(preds[:, 1]), max_y, min_y))\n",
    "\n",
    "    samples, noises = np.array(samples),  np.array(noises)\n",
    "    means = (samples.mean(axis=0)).reshape(-1)\n",
    "\n",
    "    # model precision\n",
    "    # tau = l2 * (1-p_drop) / (2*y.shape[0]*wght_decay)\n",
    "\n",
    "    aleatoric = (noises ** 2).mean(axis=0) ** 0.5\n",
    "    epistemic = (samples.var(axis=0) ** 0.5).reshape(-1)\n",
    "    total_unc = (aleatoric ** 2 + epistemic ** 2) ** 0.5\n",
    "\n",
    "#     print(\"Aleatoric uncertainty mean: {0:.4f}, Epistemic uncertainty mean: {1:.4f}, Total uncertainty mean: {2:.4f}\"\n",
    "#           .format(aleatoric.mean(), epistemic.mean(), total_unc.mean()))\n",
    "#     print(\"Aleatoric uncertainty std: {0:.4f}, Epistemic uncertainty std: {1:.4f}, Total uncertainty std: {2:.4f}\"\n",
    "#           .format(aleatoric.std(), epistemic.std(), total_unc.std()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    mu_bl = means.mean()\n",
    "    sig_bl_al = aleatoric.mean()\n",
    "    sig_bl_ep = epistemic.mean()\n",
    "    sig_bl = total_unc.mean()\n",
    "    \n",
    "    means = [mu_bl] * 3 # 3 number of sets\n",
    "    aleatoric = [sig_bl_al] * 3\n",
    "    epistemic = [sig_bl_ep] * 3\n",
    "    total_unc = [sig_bl] * 3\n",
    "    \n",
    "    # Convert built Python lists to a Numpy array.\n",
    "    means = np.array(means, dtype='float32')\n",
    "    aleatoric = np.array(aleatoric, dtype='float32')\n",
    "    epistemic = np.array(epistemic, dtype='float32')\n",
    "    total_unc = np.array(total_unc, dtype='float32')\n",
    "    \n",
    "    print(means,aleatoric,epistemic,total_unc)\n",
    "    \n",
    "    \n",
    "#     # Dimensionless BL: non-dimensionalize the BL by dividing with the layer height\n",
    "#     dimensionless_mean_bl = means.mean()/height\n",
    "#     dimensionless_total_unc_bl = total_unc.mean()/height**2\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # denormalize test data\n",
    "    x_tst = denormalize_max_min(x_tst, max_x, min_x)\n",
    "    y_tst = denormalize_max_min(y_tst, max_y, min_y)\n",
    "    \n",
    "    # mean bl of all rounds\n",
    "    y_tst_mean_rounds = y_tst.mean(axis=0)\n",
    "    \n",
    "    # mean bl overall of each round of part 25\n",
    "    y_test_overall_each = y_tst.mean(axis=1)\n",
    "    \n",
    "    # mean bl of overall part 25\n",
    "    y_test_overall = y_tst_mean_rounds.mean(axis=0)\n",
    "    \n",
    "    print(y_test_overall, y_test_overall_each)    \n",
    "\n",
    "    \n",
    "    # PLOT FIGURES\n",
    "    c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
    "         '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "\n",
    "    xx = ['Set 1','Set 2','Set 3']\n",
    "    \n",
    "    # Only plot some portion of the test data\n",
    "    fig = plt.figure()\n",
    "    plt.style.use('default')\n",
    "    plt.plot(xx, y_test_overall_each, 'b.', label='Observations');\n",
    "    plt.fill_between(xx, means + epistemic, means + total_unc, color=c[9], alpha=0.5,\n",
    "                     label='Aleatoric')\n",
    "    plt.fill_between(xx, means - total_unc, means - epistemic, color=c[9], alpha=0.5)\n",
    "    plt.fill_between(xx, means - epistemic, means + epistemic, color=c[5], alpha=0.6,\n",
    "                     label='Epistemic')\n",
    "    plt.fill_between(xx, means + total_unc, means + 2*total_unc, color=c[9], alpha=0.25)\n",
    "    plt.fill_between(xx, means - total_unc, means - 2*total_unc, color=c[9], alpha=0.25)\n",
    "    plt.fill_between(xx, means + 2*total_unc, means + 3*total_unc, color=c[9], alpha=0.1)\n",
    "    plt.fill_between(xx, means - 2*total_unc, means - 3*total_unc, color=c[9], alpha=0.1)\n",
    "    plt.plot(xx, means, color='black', linewidth=1, label='Predictive mean')\n",
    "    plt.xlabel('Sets', fontsize=16)\n",
    "    plt.ylabel('Average bond length (mm)', fontsize=16)\n",
    "    plt.legend()\n",
    "    filename='Part25_Predictions'\n",
    "    fig.savefig(\"{}.pdf\".format(filename), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Do an exhaustive search if number of units and dropout probabilities are lists\n",
    "if type(nb_units) is list and type(p_drop_rate) is list:\n",
    "    for nb_unit in nb_units:\n",
    "        print('\\n Number of units: ', nb_unit)\n",
    "        for p_drop in p_drop_rate:\n",
    "            print('\\n Dropout prob.: ', p_drop)\n",
    "            main(p_drop, nb_unit)\n",
    "elif type(nb_units) is list:\n",
    "    for nb_unit in nb_units:\n",
    "        print('\\n Number of units: ', nb_unit)\n",
    "        main(p_drop_rate, nb_unit)\n",
    "elif type(p_drop_rate) is list:\n",
    "    for p_drop in p_drop_rate:\n",
    "        print('\\n Dropout prob.: ', p_drop)\n",
    "        main(p_drop, nb_units)\n",
    "else:\n",
    "    main(p_drop_rate, nb_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
